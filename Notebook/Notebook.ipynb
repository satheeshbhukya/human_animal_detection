{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t84o0MlXKqSI",
        "outputId": "4ec3b941-2ab4-4001-fe10-030f3999c6a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PisGIxQnKsyG",
        "outputId": "a7dbb3b2-8b5b-440a-8d44-768c2d78c4a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project directory created at:\n",
            "/content/drive/MyDrive/human_animal_detection\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_NAME = \"human_animal_detection\"   # change if needed\n",
        "BASE_DIR = f\"/content/drive/MyDrive/{PROJECT_NAME}\"\n",
        "DATASET_DIR = f\"{BASE_DIR}/datasets\"\n",
        "\n",
        "os.makedirs(DATASET_DIR, exist_ok=True)\n",
        "\n",
        "print(\"Project directory created at:\")\n",
        "print(BASE_DIR)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Download dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCiIJt1GLPB_",
        "outputId": "2f802095-bea1-4399-85ca-6043e17a4861"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/glob2/fnmatch.py:141: SyntaxWarning: invalid escape sequence '\\Z'\n",
            "  return '(?ms)' + res + '\\Z'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading split 'train' to '/root/fiftyone/open-images-v7/train' if necessary\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Downloading split 'train' to '/root/fiftyone/open-images-v7/train' if necessary\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv' to '/root/fiftyone/open-images-v7/train/metadata/image_ids.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/2018_04/train/train-images-boxable-with-rotation.csv' to '/root/fiftyone/open-images-v7/train/metadata/image_ids.csv'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |██████|    4.8Gb/4.8Gb [4.0s elapsed, 0s remaining, 1.5Gb/s]         \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |██████|    4.8Gb/4.8Gb [4.0s elapsed, 0s remaining, 1.5Gb/s]         \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v7/train/metadata/classes.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v5/class-descriptions-boxable.csv' to '/root/fiftyone/open-images-v7/train/metadata/classes.csv'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmpdwy2vg8j/metadata/hierarchy.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/2018_04/bbox_labels_600_hierarchy.json' to '/tmp/tmpdwy2vg8j/metadata/hierarchy.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv' to '/root/fiftyone/open-images-v7/train/labels/detections.csv'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 'https://storage.googleapis.com/openimages/v6/oidv6-train-annotations-bbox.csv' to '/root/fiftyone/open-images-v7/train/labels/detections.csv'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 5000 images\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.utils.openimages:Downloading 5000 images\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |█████████████████| 5000/5000 [9.2m elapsed, 0s remaining, 8.6 files/s]       \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 5000/5000 [9.2m elapsed, 0s remaining, 8.6 files/s]       \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset info written to '/root/fiftyone/open-images-v7/info.json'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset info written to '/root/fiftyone/open-images-v7/info.json'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading 'open-images-v7' split 'train'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Loading 'open-images-v7' split 'train'\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |███████████████| 5000/5000 [55.3s elapsed, 0s remaining, 130.7 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 5000/5000 [55.3s elapsed, 0s remaining, 130.7 samples/s]      \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset 'open-images-v7-train-5000' created\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:fiftyone.zoo.datasets:Dataset 'open-images-v7-train-5000' created\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Downloaded 5000 images\n"
          ]
        }
      ],
      "source": [
        "import fiftyone as fo\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "classes = [\n",
        "    # Humans / Person-related\n",
        "    \"Person\", \"Man\", \"Woman\", \"Boy\", \"Girl\",\n",
        "    \"Human body\", \"Human head\", \"Human face\",\n",
        "    \"Human arm\", \"Human hand\", \"Human leg\",\n",
        "    \"Human foot\", \"Human eye\", \"Human mouth\",\n",
        "    \"Human nose\", \"Human hair\", \"Human beard\",\n",
        "\n",
        "    # Animals\n",
        "    \"Dog\", \"Cat\", \"Elephant\", \"Squirrel\", \"Crab\",\n",
        "    \"Insect\", \"Carnivore\", \"Shellfish\", \"Bird\", \"Fish\",\n",
        "    \"Horse\", \"Goat\", \"Pig\", \"Rabbit\", \"Sheep\",\n",
        "    \"Duck\", \"Sea lion\", \"Whale\", \"Dolphin\", \"Tortoise\",\n",
        "    \"Marine mammal\", \"Lion\", \"Giraffe\", \"Bat (Animal)\", \"Fox\"\n",
        "]\n",
        "\n",
        "\n",
        "import fiftyone.zoo as foz\n",
        "\n",
        "dataset = foz.load_zoo_dataset(\n",
        "    \"open-images-v7\",\n",
        "    split=\"train\",\n",
        "    label_types=[\"detections\"],\n",
        "    classes=classes,\n",
        "    max_samples=5000,\n",
        ")\n",
        "\n",
        "\n",
        "print(f\"✓ Downloaded {len(dataset)} images\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeNac6xALVaA",
        "outputId": "b63d51e0-8bcd-4b94-e5d4-f96c98f260e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " 100% |███████████████| 5000/5000 [1.5m elapsed, 0s remaining, 67.9 samples/s]      \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 5000/5000 [1.5m elapsed, 0s remaining, 67.9 samples/s]      \n"
          ]
        }
      ],
      "source": [
        "EXPORT_DIR = os.path.join(DATASET_DIR, \"train\")\n",
        "dataset.export(\n",
        "    export_dir=EXPORT_DIR,\n",
        "    dataset_type=fo.types.COCODetectionDataset,\n",
        "    label_field=\"ground_truth\",\n",
        "    overwrite=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76MzLts8O9sp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "PROJECT_NAME = \"human_animal_detection\"   # change if needed\n",
        "BASE_DIR = f\"/content/drive/MyDrive/{PROJECT_NAME}\"\n",
        "SCRIPTS_DIR = f\"{BASE_DIR}/scripts\"\n",
        "\n",
        "os.makedirs(SCRIPTS_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Detection codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qx6kXNqaOx08"
      },
      "outputs": [],
      "source": [
        "# Your scripts folder\n",
        "SCRIPTS_DIR = \"/content/drive/MyDrive/human_animal_detection/scripts\"\n",
        "\n",
        "# 1️⃣ Create dataset.py\n",
        "dataset_code = \"\"\"\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class HumanAnimalDataset(Dataset):\n",
        "    def __init__(self, data_dir, transform=None):\n",
        "        self.data_dir = data_dir\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load COCO-style JSON annotations\n",
        "        with open(os.path.join(data_dir, \"labels.json\")) as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        # Map category_id -> label index (start from 1, 0 is background)\n",
        "        self.cat2label = {cat[\"id\"]: i+1 for i, cat in enumerate(data[\"categories\"])}\n",
        "\n",
        "        # Map image_id -> image info\n",
        "        self.images = {img[\"id\"]: img for img in data[\"images\"]}\n",
        "\n",
        "        # Group annotations by image_id\n",
        "        self.image_id_to_ann = {}\n",
        "        for ann in data[\"annotations\"]:\n",
        "            img_id = ann[\"image_id\"]\n",
        "            if img_id not in self.image_id_to_ann:\n",
        "                self.image_id_to_ann[img_id] = []\n",
        "            self.image_id_to_ann[img_id].append(ann)\n",
        "\n",
        "        # Keep a list of image ids for indexing\n",
        "        self.ids = list(self.images.keys())\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_id = self.ids[idx]\n",
        "        img_info = self.images[image_id]\n",
        "        anns = self.image_id_to_ann.get(image_id, [])\n",
        "\n",
        "        img_path = os.path.join(self.data_dir, \"data\", img_info[\"file_name\"])\n",
        "        if not os.path.exists(img_path):\n",
        "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
        "\n",
        "        img = Image.open(img_path).convert(\"RGB\")\n",
        "\n",
        "        boxes = []\n",
        "        labels = []\n",
        "\n",
        "        for ann in anns:\n",
        "            x, y, w, h = ann[\"bbox\"]\n",
        "            boxes.append([x, y, x + w, y + h])\n",
        "            labels.append(self.cat2label[ann[\"category_id\"]])\n",
        "\n",
        "        boxes = torch.tensor(boxes, dtype=torch.float32)\n",
        "        labels = torch.tensor(labels, dtype=torch.int64)\n",
        "\n",
        "        target = {\"boxes\": boxes, \"labels\": labels}\n",
        "\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(SCRIPTS_DIR, \"dataset.py\"), \"w\") as f:\n",
        "    f.write(dataset_code)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eBp9e32ZPpWa"
      },
      "outputs": [],
      "source": [
        "# Your scripts folder\n",
        "SCRIPTS_DIR = \"/content/drive/MyDrive/human_animal_detection/scripts\"\n",
        "\n",
        "train_code = \"\"\"\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision import transforms as T\n",
        "from dataset import HumanAnimalDataset\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/human_animal_detection/datasets/train\"  # images + labels.json\n",
        "MODEL_PATH = \"/content/drive/MyDrive/human_animal_detection/models/detector_mobilenet.pth\"\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "\n",
        "# Dataset & Dataloader\n",
        "dataset = HumanAnimalDataset(DATA_DIR, transform=T.ToTensor())\n",
        "loader = DataLoader(dataset, batch_size=4, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
        "\n",
        "# Model: FasterRCNN with MobileNetV3 + FPN\n",
        "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=\"DEFAULT\")\n",
        "num_classes = len(dataset.cat2label) + 1  # background + all categories\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(\n",
        "    in_features, num_classes\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
        "\n",
        "# Training\n",
        "num_epochs = 2\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for images, targets in loader:\n",
        "        images = [img.to(device) for img in images]\n",
        "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "        loss_dict = model(images, targets)\n",
        "        loss = sum(loss_dict.values())\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {total_loss/len(loader):.4f}\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(\"Detector model saved to:\", MODEL_PATH)\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "with open(os.path.join(SCRIPTS_DIR, \"train_detector.py\"), \"w\") as f:\n",
        "    f.write(train_code)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To train detector model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMy7P1SEQF_v",
        "outputId": "c2edc515-df4b-4110-ded6-a10b5254c3b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/2, Loss: 1.6379\n",
            "Epoch 2/2, Loss: 1.3786\n",
            "Detector model saved to: /content/drive/MyDrive/human_animal_detection/models/detector_mobilenet.pth\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/human_animal_detection/scripts/train_detector.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification codes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Db3SmeYdqs9_"
      },
      "outputs": [],
      "source": [
        "# Your scripts folder\n",
        "SCRIPTS_DIR = \"/content/drive/MyDrive/human_animal_detection/scripts\"\n",
        "\n",
        "dataset_code = \"\"\"\n",
        "import os\n",
        "import json\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "HUMAN_KEYWORDS = [\n",
        "    \"person\", \"man\", \"woman\", \"boy\", \"girl\", \"human\"\n",
        "]\n",
        "\n",
        "ANIMAL_KEYWORDS = [\n",
        "    \"animal\", \"dog\", \"cat\", \"horse\", \"sheep\", \"pig\",\n",
        "    \"goat\", \"rabbit\", \"lion\", \"fox\", \"bat\",\n",
        "    \"bird\", \"fish\", \"whale\", \"dolphin\", \"tortoise\",\n",
        "    \"squirrel\", \"crab\", \"shellfish\", \"marine mammal\",\n",
        "    \"sea lion\", \"insect\", \"giraffe\", \"elephant\"\n",
        "]\n",
        "\n",
        "class HumanAnimalClassificationDataset(Dataset):\n",
        "    \\\"\"\"\n",
        "    0 -> Human\n",
        "    1 -> Animal\n",
        "    \\\"\"\"\n",
        "\n",
        "    def __init__(self, root_dir, transform=None):\n",
        "        self.image_dir = os.path.join(root_dir, \"data\")\n",
        "        self.transform = transform\n",
        "\n",
        "        with open(os.path.join(root_dir, \"labels.json\")) as f:\n",
        "            coco = json.load(f)\n",
        "\n",
        "        cat_id_to_name = {\n",
        "            cat[\"id\"]: cat[\"name\"].lower() for cat in coco[\"categories\"]\n",
        "        }\n",
        "\n",
        "        image_id_to_name = {\n",
        "            img[\"id\"]: img[\"file_name\"] for img in coco[\"images\"]\n",
        "        }\n",
        "\n",
        "        image_labels = {}\n",
        "        for ann in coco[\"annotations\"]:\n",
        "            img_id = ann[\"image_id\"]\n",
        "            cat_name = cat_id_to_name[ann[\"category_id\"]]\n",
        "            image_labels.setdefault(img_id, set()).add(cat_name)\n",
        "\n",
        "        self.samples = []\n",
        "        for img_id, cats in image_labels.items():\n",
        "            label = None\n",
        "            if any(any(h in c for h in HUMAN_KEYWORDS) for c in cats):\n",
        "                label = 0\n",
        "            elif any(any(a in c for a in ANIMAL_KEYWORDS) for c in cats):\n",
        "                label = 1\n",
        "\n",
        "            if label is not None:\n",
        "                img_path = os.path.join(self.image_dir, image_id_to_name[img_id])\n",
        "                if os.path.exists(img_path):\n",
        "                    self.samples.append((img_path, label))\n",
        "\n",
        "        print(f\"Loaded {len(self.samples)} classification samples\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, label = self.samples[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "with open(os.path.join(SCRIPTS_DIR, \"dataset_cls.py\"), \"w\") as f:\n",
        "    f.write(dataset_code)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "lXTrs2IvqssF"
      },
      "outputs": [],
      "source": [
        "# Your scripts folder\n",
        "SCRIPTS_DIR = \"/content/drive/MyDrive/human_animal_detection/scripts\"\n",
        "\n",
        "train_code = \"\"\"\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, models\n",
        "from dataset_cls import HumanAnimalClassificationDataset\n",
        "\n",
        "# Device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Paths\n",
        "DATA_DIR = \"/content/drive/MyDrive/human_animal_detection/datasets/train\"\n",
        "MODEL_PATH = \"/content/drive/MyDrive/human_animal_detection/models/classifier_resnet18.pth\"\n",
        "os.makedirs(os.path.dirname(MODEL_PATH), exist_ok=True)\n",
        "\n",
        "# Transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Dataset & Dataloader\n",
        "dataset = HumanAnimalClassificationDataset(DATA_DIR, transform=transform)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=2)\n",
        "\n",
        "# Model: ResNet18\n",
        "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "model.fc = nn.Linear(model.fc.in_features, 2)  # human vs animal\n",
        "model.to(device)\n",
        "\n",
        "# Loss & Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "\n",
        "# Training\n",
        "epochs = 2\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        preds = outputs.argmax(dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    acc = 100 * correct / total\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {running_loss/len(loader):.4f} | Acc: {acc:.2f}%\")\n",
        "\n",
        "# Save model\n",
        "torch.save(model.state_dict(), MODEL_PATH)\n",
        "print(\"Classifier saved to:\", MODEL_PATH)\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(SCRIPTS_DIR, \"train_classifier.py\"), \"w\") as f:\n",
        "    f.write(train_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-RPf1jgrfoM",
        "outputId": "8b0a8c26-1217-4e29-9a0f-57c4ef8ed752"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 4924 classification samples\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100% 44.7M/44.7M [00:00<00:00, 179MB/s]\n",
            "Epoch 1/2 | Loss: 0.2059 | Acc: 92.22%\n",
            "Epoch 2/2 | Loss: 0.0357 | Acc: 99.07%\n",
            "Classifier saved to: /content/drive/MyDrive/human_animal_detection/models/classifier_resnet18.pth\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/human_animal_detection/scripts/train_classifier.py\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Inference code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "kQX8QrwOxoiD"
      },
      "outputs": [],
      "source": [
        "# Your scripts folder\n",
        "SCRIPTS_DIR = \"/content/drive/MyDrive/human_animal_detection/scripts\"\n",
        "\n",
        "infer_code = \"\"\"\n",
        "# scripts/inference.py\n",
        "# scripts/inference_video.py\n",
        "import os\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "import torchvision.models as models\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "import cv2\n",
        "from dataset import HumanAnimalDataset  # for num_classes info if needed\n",
        "\n",
        "# ---------------- Paths ---------------- #\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DETECTOR_PATH = \"/content/drive/MyDrive/human_animal_detection/models/detector_mobilenet.pth\"\n",
        "CLASSIFIER_PATH = \"/content/drive/MyDrive/human_animal_detection/models/classifier_resnet18.pth\"\n",
        "VIDEO_PATH = \"/content/drive/MyDrive/human_animal_detection/test_video/video.mp4\"\n",
        "OUTPUT_PATH = \"/content/drive/MyDrive/human_animal_detection/test_video/output.mp4\"\n",
        "\n",
        "# ---------------- Load Detector ---------------- #\n",
        "dataset_info = HumanAnimalDataset(\"/content/drive/MyDrive/human_animal_detection/datasets/train\")\n",
        "num_classes = len(dataset_info.cat2label) + 1  # background + all categories\n",
        "\n",
        "detector = models.detection.fasterrcnn_mobilenet_v3_large_fpn(weights=None)\n",
        "in_features = detector.roi_heads.box_predictor.cls_score.in_features\n",
        "detector.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "detector.load_state_dict(torch.load(DETECTOR_PATH, map_location=DEVICE))\n",
        "detector.to(DEVICE)\n",
        "detector.eval()\n",
        "\n",
        "# ---------------- Load Classifier ---------------- #\n",
        "classifier = models.resnet18(weights=None)\n",
        "classifier.fc = torch.nn.Linear(classifier.fc.in_features, 2)  # 0: human, 1: animal\n",
        "classifier.load_state_dict(torch.load(CLASSIFIER_PATH, map_location=DEVICE))\n",
        "classifier.to(DEVICE)\n",
        "classifier.eval()\n",
        "\n",
        "# ---------------- Video Setup ---------------- #\n",
        "cap = cv2.VideoCapture(VIDEO_PATH)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(OUTPUT_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "transform = T.ToTensor()\n",
        "\n",
        "# ---------------- Inference Loop ---------------- #\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    img_tensor = transform(frame).to(DEVICE)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = detector([img_tensor])\n",
        "\n",
        "    boxes = outputs[0]['boxes']\n",
        "    scores = outputs[0]['scores']\n",
        "\n",
        "    for box, score in zip(boxes, scores):\n",
        "        if score < 0.5:\n",
        "            continue\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "\n",
        "        crop = frame[y1:y2, x1:x2]\n",
        "        if crop.shape[0] == 0 or crop.shape[1] == 0:\n",
        "            continue\n",
        "        crop_tensor = transform(crop).unsqueeze(0).to(DEVICE)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = classifier(crop_tensor).argmax(dim=1).item()\n",
        "        class_name = \"Human\" if pred == 0 else \"Animal\"\n",
        "        cv2.putText(frame, class_name, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                    0.8, (0, 0, 255), 2)\n",
        "\n",
        "    out.write(frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(\"Output video saved to:\", OUTPUT_PATH)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(SCRIPTS_DIR, \"inference.py\"), \"w\") as f:\n",
        "    f.write(infer_code)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To run inference code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMCaV1nAyI76",
        "outputId": "093ad5e1-12d9-4a3f-e6ee-f40ed99f0f05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Output video saved to: /content/drive/MyDrive/human_animal_detection/test_video/output.mp4\n"
          ]
        }
      ],
      "source": [
        "!python /content/drive/MyDrive/human_animal_detection/scripts/inference.py\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
